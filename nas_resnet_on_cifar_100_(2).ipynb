{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "nas-resnet-on-cifar-100 (2).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rgxVzOhPjbZI",
        "Wi7mStljkj3U",
        "O2YxcggZkt5G"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteobarato/sc/blob/master/nas_resnet_on_cifar_100_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch \n",
        "!pip install torchvision\n",
        "!pip install torch_pruning"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:17.328657Z",
          "iopub.execute_input": "2022-06-09T08:50:17.329050Z",
          "iopub.status.idle": "2022-06-09T08:50:44.884300Z",
          "shell.execute_reply.started": "2022-06-09T08:50:17.328997Z",
          "shell.execute_reply": "2022-06-09T08:50:44.883245Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMwdyVyXMv5K",
        "outputId": "cb49cd0f-2ed1-4689-80b6-39cb0af59571"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.2.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.11.0+cu113)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_pruning in /usr/local/lib/python3.7/dist-packages (0.2.7)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch_pruning) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torch_pruning) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downalod ResNet18 weights\n",
        "!wget -c http://cipizio.it/storage/Nas-ResNet/resnet18_net_e_199.pth\n",
        "!mkdir checkpoint\n",
        "!mv resnet18_net_e_199.pth ./checkpoint"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:44.886540Z",
          "iopub.execute_input": "2022-06-09T08:50:44.887242Z",
          "iopub.status.idle": "2022-06-09T08:50:57.426431Z",
          "shell.execute_reply.started": "2022-06-09T08:50:44.887200Z",
          "shell.execute_reply": "2022-06-09T08:50:57.425196Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PIjVwQ9Mv5P",
        "outputId": "edb1bcf3-6376-4c03-93fd-a8b2674f3701"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2022-06-14 18:53:05--  https://cipizio.it/storage/Nas-ResNet/resnet18_net_e_199.pth\n",
            "Resolving cipizio.it (cipizio.it)... 51.83.75.172\n",
            "Connecting to cipizio.it (cipizio.it)|51.83.75.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44773645 (43M) [application/octet-stream]\n",
            "Saving to: ‘resnet18_net_e_199.pth’\n",
            "\n",
            "resnet18_net_e_199. 100%[===================>]  42.70M  10.1MB/s    in 5.0s    \n",
            "\n",
            "2022-06-14 18:53:11 (8.50 MB/s) - ‘resnet18_net_e_199.pth’ saved [44773645/44773645]\n",
            "\n",
            "mkdir: cannot create directory ‘checkpoint’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "# from KDLib.vanilla_kd import GatedKD\n",
        "\n",
        "\n",
        "import torch_pruning as tp\n",
        "\n",
        "import os\n",
        "import argparse"
      ],
      "metadata": {
        "id": "sLNDUN1DjMrU",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:57:37.369292Z",
          "iopub.execute_input": "2022-06-09T08:57:37.369760Z",
          "iopub.status.idle": "2022-06-09T08:57:37.378954Z",
          "shell.execute_reply.started": "2022-06-09T08:57:37.369718Z",
          "shell.execute_reply": "2022-06-09T08:57:37.377986Z"
        },
        "trusted": true
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def progress_bar(current, total, msg=None):\n",
        "    if current >= (total -1) : print(f\"Batch {current}/{total} : {msg}\")\n",
        "\n",
        "def resume_checkpoint(net, path, map_location=torch.device('cuda') ):\n",
        "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/'+path+'.pth', map_location=map_location )\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    return  checkpoint['acc'], checkpoint['epoch']\n",
        "    # best_acc = checkpoint['acc']\n",
        "    # start_epoch = checkpoint['epoch']\n"
      ],
      "metadata": {
        "id": "tv5i7PSknLVt",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:57.437117Z",
          "iopub.execute_input": "2022-06-09T08:50:57.437980Z",
          "iopub.status.idle": "2022-06-09T08:50:57.446310Z",
          "shell.execute_reply.started": "2022-06-09T08:50:57.437942Z",
          "shell.execute_reply": "2022-06-09T08:50:57.445591Z"
        },
        "trusted": true
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "args = {'resume':False, 'lr':0.001, 'lambda_gating':0.1}\n",
        "\n",
        "if device == 'cuda':\n",
        "    # net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "JLPbE5t5jVMt",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:57.447725Z",
          "iopub.execute_input": "2022-06-09T08:50:57.448438Z",
          "iopub.status.idle": "2022-06-09T08:50:57.454626Z",
          "shell.execute_reply.started": "2022-06-09T08:50:57.448337Z",
          "shell.execute_reply": "2022-06-09T08:50:57.453813Z"
        },
        "trusted": true
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset CIFAR10"
      ],
      "metadata": {
        "id": "rgxVzOhPjbZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "metadata": {
        "id": "YnkIpnukjaUU",
        "outputId": "f7e9d536-6597-42e3-8368-d9f0a2d14045",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:57.456061Z",
          "iopub.execute_input": "2022-06-09T08:50:57.456681Z",
          "iopub.status.idle": "2022-06-09T08:50:59.146131Z",
          "shell.execute_reply.started": "2022-06-09T08:50:57.456644Z",
          "shell.execute_reply": "2022-06-09T08:50:59.145143Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet18"
      ],
      "metadata": {
        "id": "Wi7mStljkj3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''ResNet in PyTorch.\n",
        "\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n",
        "\n",
        "def freezeAllLayers(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "id": "IRq-CNWdkl65",
        "outputId": "24f6df01-105b-4087-fcbc-ee449bc38786",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:59.147780Z",
          "iopub.execute_input": "2022-06-09T08:50:59.148175Z",
          "iopub.status.idle": "2022-06-09T08:50:59.442471Z",
          "shell.execute_reply.started": "2022-06-09T08:50:59.148139Z",
          "shell.execute_reply": "2022-06-09T08:50:59.441627Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KD"
      ],
      "metadata": {
        "id": "RNj_20boMv5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "class BaseClass:\n",
        "    \"\"\"\n",
        "    Basic implementation of a general Knowledge Distillation framework\n",
        "\n",
        "    :param teacher_model (torch.nn.Module): Teacher model\n",
        "    :param student_model (torch.nn.Module): Student model\n",
        "    :param train_loader (torch.utils.data.DataLoader): Dataloader for training\n",
        "    :param val_loader (torch.utils.data.DataLoader): Dataloader for validation/testing\n",
        "    :param optimizer_teacher (torch.optim.*): Optimizer used for training teacher\n",
        "    :param optimizer_student (torch.optim.*): Optimizer used for training student\n",
        "    :param loss_fn (torch.nn.Module): Loss Function used for distillation\n",
        "    :param temp (float): Temperature parameter for distillation\n",
        "    :param distil_weight (float): Weight paramter for distillation loss\n",
        "    :param device (str): Device used for training; 'cpu' for cpu and 'cuda' for gpu\n",
        "    :param log (bool): True if logging required\n",
        "    :param logdir (str): Directory for storing logs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        teacher_model,\n",
        "        student_model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        optimizer_teacher,\n",
        "        optimizer_student,\n",
        "        loss_fn=nn.KLDivLoss(),\n",
        "        temp=20.0,\n",
        "        distil_weight=0.5,\n",
        "        device=\"cpu\",\n",
        "        log=False,\n",
        "        logdir=\"./Experiments\",\n",
        "    ):\n",
        "\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.optimizer_teacher = optimizer_teacher\n",
        "        self.optimizer_student = optimizer_student\n",
        "        self.temp = temp\n",
        "        self.distil_weight = distil_weight\n",
        "        self.log = log\n",
        "        self.logdir = logdir\n",
        "\n",
        "        if self.log:\n",
        "            self.writer = SummaryWriter(logdir)\n",
        "\n",
        "        if device == \"cpu\":\n",
        "            self.device = torch.device(\"cpu\")\n",
        "        elif device == \"cuda\":\n",
        "            if torch.cuda.is_available():\n",
        "                self.device = torch.device(\"cuda\")\n",
        "            else:\n",
        "                print(\n",
        "                    \"Either an invalid device or CUDA is not available. Defaulting to CPU.\"\n",
        "                )\n",
        "                self.device = torch.device(\"cpu\")\n",
        "\n",
        "        if teacher_model:\n",
        "            self.teacher_model = teacher_model.to(self.device)\n",
        "        else:\n",
        "            print(\"Warning!!! Teacher is NONE.\")\n",
        "\n",
        "        self.student_model = student_model.to(self.device)\n",
        "        self.loss_fn = loss_fn.to(self.device)\n",
        "        self.ce_fn = nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "    def train_teacher(\n",
        "        self,\n",
        "        epochs=20,\n",
        "        plot_losses=True,\n",
        "        save_model=True,\n",
        "        save_model_pth=\"./models/teacher.pt\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Function that will be training the teacher\n",
        "\n",
        "        :param epochs (int): Number of epochs you want to train the teacher\n",
        "        :param plot_losses (bool): True if you want to plot the losses\n",
        "        :param save_model (bool): True if you want to save the teacher model\n",
        "        :param save_model_pth (str): Path where you want to store the teacher model\n",
        "        \"\"\"\n",
        "        self.teacher_model.train()\n",
        "        loss_arr = []\n",
        "        length_of_dataset = len(self.train_loader.dataset)\n",
        "        best_acc = 0.0\n",
        "        self.best_teacher_model_weights = deepcopy(self.teacher_model.state_dict())\n",
        "\n",
        "        save_dir = os.path.dirname(save_model_pth)\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        print(\"Training Teacher... \")\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            epoch_loss = 0.0\n",
        "            correct = 0\n",
        "            for (data, label) in self.train_loader:\n",
        "                data = data.to(self.device)\n",
        "                label = label.to(self.device)\n",
        "                out = self.teacher_model(data)\n",
        "\n",
        "                if isinstance(out, tuple):\n",
        "                    out = out[0]\n",
        "\n",
        "                pred = out.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "\n",
        "                loss = self.ce_fn(out, label)\n",
        "\n",
        "                self.optimizer_teacher.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer_teacher.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            epoch_acc = correct / length_of_dataset\n",
        "\n",
        "            epoch_val_acc = self.evaluate(teacher=True)\n",
        "\n",
        "            if epoch_val_acc > best_acc:\n",
        "                best_acc = epoch_val_acc\n",
        "                self.best_teacher_model_weights = deepcopy(\n",
        "                    self.teacher_model.state_dict()\n",
        "                )\n",
        "\n",
        "            if self.log:\n",
        "                self.writer.add_scalar(\"Training loss/Teacher\", epoch_loss, epochs)\n",
        "                self.writer.add_scalar(\"Training accuracy/Teacher\", epoch_acc, epochs)\n",
        "                self.writer.add_scalar(\n",
        "                    \"Validation accuracy/Teacher\", epoch_val_acc, epochs\n",
        "                )\n",
        "\n",
        "            loss_arr.append(epoch_loss)\n",
        "            print(\n",
        "                \"Epoch: {}, Loss: {}, Accuracy: {}\".format(\n",
        "                    ep + 1, epoch_loss, epoch_acc\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.post_epoch_call(ep)\n",
        "\n",
        "        self.teacher_model.load_state_dict(self.best_teacher_model_weights)\n",
        "        if save_model:\n",
        "            self.save_checkpoint(self.teacher_model, epoch_acc, epochs, save_model_pth)\n",
        "        if plot_losses:\n",
        "            plt.plot(loss_arr)\n",
        "\n",
        "    def _train_student(\n",
        "        self,\n",
        "        epochs=10,\n",
        "        plot_losses=True,\n",
        "        save_model=True,\n",
        "        save_model_pth=\"./models/student.pt\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Function to train student model - for internal use only.\n",
        "\n",
        "        :param epochs (int): Number of epochs you want to train the teacher\n",
        "        :param plot_losses (bool): True if you want to plot the losses\n",
        "        :param save_model (bool): True if you want to save the student model\n",
        "        :param save_model_pth (str): Path where you want to save the student model\n",
        "        \"\"\"\n",
        "        self.teacher_model.eval()\n",
        "        self.student_model.train()\n",
        "        loss_arr = []\n",
        "        length_of_dataset = len(self.train_loader.dataset)\n",
        "        best_acc = 0.0\n",
        "        self.best_student_model_weights = deepcopy(self.student_model.state_dict())\n",
        "\n",
        "        save_dir = os.path.dirname(save_model_pth)\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "        print(\"Training Student...\")\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            epoch_loss = 0.0\n",
        "            correct = 0\n",
        "\n",
        "            for (data, label) in self.train_loader:\n",
        "\n",
        "                data = data.to(self.device)\n",
        "                label = label.to(self.device)\n",
        "\n",
        "                student_out = self.student_model(data)\n",
        "                teacher_out = self.teacher_model(data)\n",
        "\n",
        "                loss = self.calculate_kd_loss(student_out, teacher_out, label)\n",
        "\n",
        "                if isinstance(student_out, tuple):\n",
        "                    student_out = student_out[0]\n",
        "\n",
        "                pred = student_out.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "\n",
        "                self.optimizer_student.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer_student.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            epoch_acc = correct / length_of_dataset\n",
        "\n",
        "            _, epoch_val_acc = self._evaluate_model(self.student_model, verbose=True)\n",
        "\n",
        "            if epoch_val_acc > best_acc:\n",
        "                best_acc = epoch_val_acc\n",
        "                self.best_student_model_weights = deepcopy(\n",
        "                    self.student_model.state_dict()\n",
        "                )\n",
        "\n",
        "            if self.log:\n",
        "                self.writer.add_scalar(\"Training loss/Student\", epoch_loss, epochs)\n",
        "                self.writer.add_scalar(\"Training accuracy/Student\", epoch_acc, epochs)\n",
        "                self.writer.add_scalar(\n",
        "                    \"Validation accuracy/Student\", epoch_val_acc, epochs\n",
        "                )\n",
        "\n",
        "            loss_arr.append(epoch_loss)\n",
        "            print(\n",
        "                \"Epoch: {}, Loss: {}, Accuracy: {}\".format(\n",
        "                    ep + 1, epoch_loss, epoch_acc\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.student_model.load_state_dict(self.best_student_model_weights)\n",
        "        if save_model:\n",
        "            self.save_checkpoint(self.student_model, epoch_acc, epochs, save_model_pth)\n",
        "        if plot_losses:\n",
        "            plt.plot(loss_arr)\n",
        "\n",
        "    def save_checkpoint(net, acc, epoch, save_model_pth):\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        torch.save(state, save_model_pth)\n",
        "\n",
        "    def train_student(\n",
        "        self,\n",
        "        epochs=10,\n",
        "        plot_losses=True,\n",
        "        save_model=True,\n",
        "        save_model_pth=\"./models/student.pt\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Function that will be training the student\n",
        "\n",
        "        :param epochs (int): Number of epochs you want to train the teacher\n",
        "        :param plot_losses (bool): True if you want to plot the losses\n",
        "        :param save_model (bool): True if you want to save the student model\n",
        "        :param save_model_pth (str): Path where you want to save the student model\n",
        "        \"\"\"\n",
        "        self._train_student(epochs, plot_losses, save_model, save_model_pth)\n",
        "\n",
        "    def calculate_kd_loss(self, y_pred_student, y_pred_teacher, y_true):\n",
        "        \"\"\"\n",
        "        Custom loss function to calculate the KD loss for various implementations\n",
        "\n",
        "        :param y_pred_student (Tensor): Predicted outputs from the student network\n",
        "        :param y_pred_teacher (Tensor): Predicted outputs from the teacher network\n",
        "        :param y_true (Tensor): True labels\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _evaluate_model(self, model, verbose=True):\n",
        "        \"\"\"\n",
        "        Evaluate the given model's accuaracy over val set.\n",
        "        For internal use only.\n",
        "\n",
        "        :param model (nn.Module): Model to be used for evaluation\n",
        "        :param verbose (bool): Display Accuracy\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        length_of_dataset = len(self.val_loader.dataset)\n",
        "        correct = 0\n",
        "        outputs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in self.val_loader:\n",
        "                data = data.to(self.device)\n",
        "                target = target.to(self.device)\n",
        "                output = model(data)\n",
        "\n",
        "                if isinstance(output, tuple):\n",
        "                    output = output[0]\n",
        "                outputs.append(output)\n",
        "\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        accuracy = correct / length_of_dataset\n",
        "\n",
        "        if verbose:\n",
        "            print(\"-\" * 80)\n",
        "            print(\"Validation Accuracy: {}\".format(accuracy))\n",
        "        return outputs, accuracy\n",
        "\n",
        "    def evaluate(self, teacher=False):\n",
        "        \"\"\"\n",
        "        Evaluate method for printing accuracies of the trained network\n",
        "\n",
        "        :param teacher (bool): True if you want accuracy of the teacher network\n",
        "        \"\"\"\n",
        "        if teacher:\n",
        "            model = deepcopy(self.teacher_model).to(self.device)\n",
        "        else:\n",
        "            model = deepcopy(self.student_model).to(self.device)\n",
        "        _, accuracy = self._evaluate_model(model)\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def get_parameters(self):\n",
        "        \"\"\"\n",
        "        Get the number of parameters for the teacher and the student network\n",
        "        \"\"\"\n",
        "        teacher_params = sum(p.numel() for p in self.teacher_model.parameters())\n",
        "        student_params = sum(p.numel() for p in self.student_model.parameters())\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "        print(\"Total parameters for the teacher network are: {}\".format(teacher_params))\n",
        "        print(\"Total parameters for the student network are: {}\".format(student_params))\n",
        "\n",
        "    def post_epoch_call(self, epoch):\n",
        "        \"\"\"\n",
        "        Any changes to be made after an epoch is completed.\n",
        "\n",
        "        :param epoch (int) : current epoch number\n",
        "        :return            : nothing (void)\n",
        "        \"\"\"\n",
        "\n",
        "        pass\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:59.444056Z",
          "iopub.execute_input": "2022-06-09T08:50:59.444584Z",
          "iopub.status.idle": "2022-06-09T08:50:59.486055Z",
          "shell.execute_reply.started": "2022-06-09T08:50:59.444545Z",
          "shell.execute_reply": "2022-06-09T08:50:59.485251Z"
        },
        "trusted": true,
        "id": "NnZnjbPrMv5c"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KD Losses"
      ],
      "metadata": {
        "id": "qjtrbtBlrjoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VanillaKD(BaseClass):\n",
        "    \"\"\"\n",
        "    Original implementation of Knowledge distillation from the paper \"Distilling the\n",
        "    Knowledge in a Neural Network\" https://arxiv.org/pdf/1503.02531.pdf\n",
        "\n",
        "    :param teacher_model (torch.nn.Module): Teacher model\n",
        "    :param student_model (torch.nn.Module): Student model\n",
        "    :param train_loader (torch.utils.data.DataLoader): Dataloader for training\n",
        "    :param val_loader (torch.utils.data.DataLoader): Dataloader for validation/testing\n",
        "    :param optimizer_teacher (torch.optim.*): Optimizer used for training teacher\n",
        "    :param optimizer_student (torch.optim.*): Optimizer used for training student\n",
        "    :param loss_fn (torch.nn.Module):  Calculates loss during distillation\n",
        "    :param temp (float): Temperature parameter for distillation\n",
        "    :param distil_weight (float): Weight paramter for distillation loss\n",
        "    :param device (str): Device used for training; 'cpu' for cpu and 'cuda' for gpu\n",
        "    :param log (bool): True if logging required\n",
        "    :param logdir (str): Directory for storing logs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        teacher_model,\n",
        "        student_model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        optimizer_teacher,\n",
        "        optimizer_student,\n",
        "        loss_fn=nn.MSELoss(),\n",
        "        temp=20.0,\n",
        "        distil_weight=0.5,\n",
        "        device=\"cpu\",\n",
        "        log=False,\n",
        "        logdir=\"./Experiments\",\n",
        "    ):\n",
        "        super(VanillaKD, self).__init__(\n",
        "            teacher_model,\n",
        "            student_model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            optimizer_teacher,\n",
        "            optimizer_student,\n",
        "            loss_fn,\n",
        "            temp,\n",
        "            distil_weight,\n",
        "            device,\n",
        "            log,\n",
        "            logdir,\n",
        "        )\n",
        "\n",
        "    def calculate_kd_loss(self, y_pred_student, y_pred_teacher, y_true):\n",
        "        \"\"\"\n",
        "        Function used for calculating the KD loss during distillation\n",
        "\n",
        "        :param y_pred_student (torch.FloatTensor): Prediction made by the student model\n",
        "        :param y_pred_teacher (torch.FloatTensor): Prediction made by the teacher model\n",
        "        :param y_true (torch.FloatTensor): Original label\n",
        "        \"\"\"\n",
        "\n",
        "        soft_teacher_out = F.softmax(y_pred_teacher / self.temp, dim=1)\n",
        "        soft_student_out = F.softmax(y_pred_student / self.temp, dim=1)\n",
        "\n",
        "        loss = (1 - self.distil_weight) * F.cross_entropy(y_pred_student, y_true)\n",
        "        loss += (self.distil_weight * self.temp * self.temp) * self.loss_fn(\n",
        "            soft_teacher_out, soft_student_out\n",
        "        )\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class GatedKD(BaseClass):\n",
        "    \"\"\"\n",
        "    Original implementation of Knowledge distillation from the paper \"Distilling the\n",
        "    Knowledge in a Neural Network\" https://arxiv.org/pdf/1503.02531.pdf\n",
        "\n",
        "    :param teacher_model (torch.nn.Module): Teacher model\n",
        "    :param student_model (torch.nn.Module): Student model\n",
        "    :param train_loader (torch.utils.data.DataLoader): Dataloader for training\n",
        "    :param val_loader (torch.utils.data.DataLoader): Dataloader for validation/testing\n",
        "    :param optimizer_teacher (torch.optim.*): Optimizer used for training teacher\n",
        "    :param optimizer_student (torch.optim.*): Optimizer used for training student\n",
        "    :param loss_fn (torch.nn.Module):  Calculates loss during distillation\n",
        "    :param temp (float): Temperature parameter for distillation\n",
        "    :param distil_weight (float): Weight paramter for distillation loss\n",
        "    :param device (str): Device used for training; 'cpu' for cpu and 'cuda' for gpu\n",
        "    :param log (bool): True if logging required\n",
        "    :param logdir (str): Directory for storing logs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        teacher_model,\n",
        "        student_model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        optimizer_teacher,\n",
        "        optimizer_student,\n",
        "        loss_fn=nn.MSELoss(),\n",
        "        temp=20.0,\n",
        "        distil_weight=0.5,\n",
        "        gating_weight=0.01,\n",
        "        gate_layers=[],\n",
        "        device=\"cpu\",\n",
        "        log=False,\n",
        "        logdir=\"./Experiments\",\n",
        "    ):\n",
        "        super(GatedKD, self).__init__(\n",
        "            teacher_model,\n",
        "            student_model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            optimizer_teacher,\n",
        "            optimizer_student,\n",
        "            loss_fn,\n",
        "            temp,\n",
        "            distil_weight,\n",
        "            device,\n",
        "            log,\n",
        "            logdir,\n",
        "        )\n",
        "        self.gating_weight = gating_weight\n",
        "        self.gate_layers = gate_layers\n",
        "\n",
        "    def l1_penalty(self):\n",
        "        loss = 0\n",
        "        for g in self.gate_layers :\n",
        "            values = torch.cat([x.view(-1) for x in g[0].parameters()])\n",
        "            loss += torch.norm(values, 1)\n",
        "        loss = loss / len(self.gate_layers)\n",
        "        return loss\n",
        "\n",
        "    def std_penalty(self):\n",
        "        loss = 0\n",
        "        for g in self.gate_layers :\n",
        "            values = torch.cat([x.view(-1) for x in g[0].parameters()])\n",
        "            loss += (1/(torch.std(values)+1e-4)) / 1e3\n",
        "        loss = loss / len(self.gate_layers)\n",
        "        return loss\n",
        "\n",
        "    def kd_penalty(self, rho=0.05):\n",
        "        loss = 0\n",
        "        for g in self.gate_layers :\n",
        "            p_hat = torch.cat([g[0].parameters()])#torch.cat([x.view(-1) for x in g[0].parameters()])\n",
        "            print(p_hat)\n",
        "            funcs = nn.Sigmoid()\n",
        "            p_hat = torch.mean(funcs(p_hat),1)\n",
        "            p_tensor = torch.Tensor([rho] * len(p_hat)).to(device)\n",
        "            loss += torch.sum(p_tensor * torch.log(p_tensor) - p_tensor * torch.log(p_hat) + (1 - p_tensor) * torch.log(1 - p_tensor) - (1 - p_tensor) * torch.log(1 - p_hat))\n",
        "        return loss\n",
        "\n",
        "    def calculate_kd_loss(self, y_pred_student, y_pred_teacher, y_true):\n",
        "        \"\"\"\n",
        "        Function used for calculating the KD loss during distillation\n",
        "\n",
        "        :param y_pred_student (torch.FloatTensor): Prediction made by the student model\n",
        "        :param y_pred_teacher (torch.FloatTensor): Prediction made by the teacher model\n",
        "        :param y_true (torch.FloatTensor): Original label\n",
        "        \"\"\"\n",
        "\n",
        "        soft_teacher_out = F.softmax(y_pred_teacher / self.temp, dim=1)\n",
        "        soft_student_out = F.softmax(y_pred_student / self.temp, dim=1)\n",
        "\n",
        "        l1_penalty = self.gating_weight * self.l1_penalty()\n",
        "        std_penalty = self.gating_weight * self.std_penalty()\n",
        "\n",
        "        # kd_penalty = self.gating_weight * self.kd_penalty(rho=0.05)\n",
        "        loss = (1 - self.distil_weight) * F.cross_entropy(y_pred_student, y_true)\n",
        "        loss += (self.distil_weight * self.temp * self.temp) * self.loss_fn(\n",
        "            soft_teacher_out, soft_student_out\n",
        "        )\n",
        "        loss += l1_penalty + std_penalty\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:59.487430Z",
          "iopub.execute_input": "2022-06-09T08:50:59.487790Z",
          "iopub.status.idle": "2022-06-09T08:50:59.507215Z",
          "shell.execute_reply.started": "2022-06-09T08:50:59.487754Z",
          "shell.execute_reply": "2022-06-09T08:50:59.506343Z"
        },
        "trusted": true,
        "id": "eCM5h37UMv5f"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NAS"
      ],
      "metadata": {
        "id": "O2YxcggZkt5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GatedConvolution Layer\n",
        "\n",
        "class GatedConvolution(nn.Module):\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
        "    def __init__(self, n_channels, channel_first=True):\n",
        "        super().__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.out_channels = self.n_channels\n",
        "        if channel_first:\n",
        "            weight = torch.ones(self.n_channels, 1, 1)\n",
        "        else:\n",
        "            weight = torch.ones(1, 1, self.n_channels)\n",
        "        self.weight = nn.Parameter(weight)  # nn.Parameter is a Tensor that's a module parameter.\n",
        "        # initialize weight\n",
        "        nn.init.ones_(self.weight) # weight init\n",
        "        self.weight_tranformation = lambda x:x #self.sigmoid_squeezed #torch.sigmoid\n",
        "\n",
        "    def sigmoid_squeezed(self, x):\n",
        "        c1 = 10\n",
        "        c2 = 0.5\n",
        "        return 1/(1+torch.exp(c1*(-x+c2)))\n",
        "\n",
        "    def transformed_weight(self):\n",
        "        with torch.no_grad():\n",
        "            return self.weight_tranformation(self.weight) \n",
        "\n",
        "    def forward(self, x):\n",
        "        a = torch.mul(x, self.weight_tranformation(self.weight))  # w times x + b\n",
        "        return a\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'GatedConvolution(n_channels{self.n_channels})'"
      ],
      "metadata": {
        "id": "8DDz0Ze6phRp",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:59.510549Z",
          "iopub.execute_input": "2022-06-09T08:50:59.511059Z",
          "iopub.status.idle": "2022-06-09T08:50:59.521671Z",
          "shell.execute_reply.started": "2022-06-09T08:50:59.511009Z",
          "shell.execute_reply": "2022-06-09T08:50:59.520848Z"
        },
        "trusted": true
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class NAS_GatedConvolution:\n",
        "    def __init__(self, verbose=False):\n",
        "        self.verbose = verbose\n",
        "        self.gate_layers = []\n",
        "        self.apply_to_layer_types = nn.Conv2d\n",
        "        self.max_recursion_level = 25\n",
        "        self.gating_threshold = 1e-4\n",
        "        self.ready_to_prune = []\n",
        "\n",
        "    def factory_inject_layer(self, module):\n",
        "        n_channels = module.out_channels\n",
        "        gate = GatedConvolution(n_channels)\n",
        "        return gate, nn.Sequential(module,gate)\n",
        "\n",
        "    def apply_gates_to_model(self, model, apply_to_layer_types=[], _level=0): \n",
        "        if _level >= self.max_recursion_level: \n",
        "            print(\"|ERR| Maximum level of recursion reached \", self.max_recursion_level )\n",
        "            return []\n",
        "        if hasattr(model, 'gating_layers'): print(\"|WARNING| NAS already applied to model!\")\n",
        "        if self.verbose: print(\"-\"*50, \" Level \",_level)\n",
        "        # if self.verbose: print(\"Step model\", model)\n",
        "        modules = list(model.named_children())\n",
        "        self.gate_layers = []\n",
        "        for i, module in enumerate(modules):\n",
        "            module_name, module_obj = module\n",
        "            module_children = list(module_obj.children())\n",
        "        \n",
        "            if any([isinstance(module_obj, x) for x in apply_to_layer_types]):\n",
        "                gate, gated_module = self.factory_inject_layer(module_obj)\n",
        "                if isinstance(model, nn.Sequential):\n",
        "                    model[i] = gated_module\n",
        "                else:\n",
        "                    setattr(model, module_name, gated_module)\n",
        "                setattr(gated_module, '_is_gated', True)\n",
        "                if self.verbose: print(\"Added new Layer to module \", module_name)\n",
        "                self.gate_layers.append((gate, module_name, model))\n",
        "            elif len(module_children) and not hasattr(module_obj, '_is_gated'):\n",
        "                self.gate_layers += self.apply_gates_to_model(module_obj, apply_to_layer_types=apply_to_layer_types, _level=_level+1)   \n",
        "        return self.gate_layers\n",
        "\n",
        "    def estimate_required_channels(self, use_mean = .8 ):\n",
        "        gates_zeros_idxs = []\n",
        "        with torch.no_grad():\n",
        "            for i, layer in enumerate(self.gate_layers):\n",
        "                gate, _, _ = layer\n",
        "                w = gate.transformed_weight()\n",
        "                threshold = torch.mean(w) * use_mean if use_mean else self.gating_threshold\n",
        "                are_zeros = torch.where(w < threshold, 0., 1.)\n",
        "                zeros_idxs = torch.squeeze((are_zeros-1).nonzero())[:,0]\n",
        "                gates_zeros_idxs.append(zeros_idxs.detach().cpu().tolist())\n",
        "                zeros = torch.squeeze(torch.count_nonzero(are_zeros -1 , dim=0)).item()\n",
        "                if self.verbose: print(i, \"Layer index \", i ,\" | Zeros\", zeros , \"/\", w.shape[0], \" | Mean\", torch.mean(w, dim=0).item(), \" | Std\", torch.std(w, dim=0).item())\n",
        "        return gates_zeros_idxs\n",
        "\n",
        "    def project_gates_on_model(self, use_mean=.8):\n",
        "        gates_zeros_idxs = self.estimate_required_channels(use_mean=use_mean)\n",
        "        with torch.no_grad():\n",
        "            for i, item in enumerate(zip(self.gate_layers, gates_zeros_idxs)):\n",
        "                layer, zeros_idxs = item\n",
        "                gate, module_name, model = layer\n",
        "                submodules = [x for x in model.named_children() if hasattr(x[1], \"_is_gated\")]\n",
        "                for submodule in submodules:\n",
        "                    submodule_name, submodule_obj = submodule\n",
        "                    if isinstance(submodule_obj, nn.Conv2d) and submodule_name == module_name:\n",
        "                        target_module = submodule_obj\n",
        "                        for i in zeros_idxs:\n",
        "                            target_module.weight[i,:,:] = 0\n",
        "    \n",
        "    def remove_gates_from_model(self):\n",
        "        with torch.no_grad():\n",
        "            for i, layer in enumerate(self.gate_layers):\n",
        "                gate, module_name, model = layer\n",
        "                submodules = [x for x in model.named_children() if hasattr(x[1], \"_is_gated\")]\n",
        "                for submodule in submodules:\n",
        "                    submodule_name, submodule_obj = submodule\n",
        "                    if isinstance(submodule_obj[0], nn.Conv2d) and submodule_name == module_name:\n",
        "                        target_module = submodule_obj[0]\n",
        "                        print(\"Replacing \", module_name, \" with \", target_module)\n",
        "                        setattr(model, module_name, target_module)\n",
        "                        self.ready_to_prune.append((module_name, target_module))\n",
        "\n",
        "    def optimize(self, model, use_mean=0.8):\n",
        "        gates_zeros_idxs = self.estimate_required_channels(use_mean=use_mean)\n",
        "        self.project_gates_on_model(use_mean=use_mean)\n",
        "        self.remove_gates_from_model()\n",
        "        self.prune_model_channels(model, amount=0.5, pruning_idxs=gates_zeros_idxs)\n",
        "        return gates_zeros_idxs\n",
        "\n",
        "    def calc_improvement(self, base_model , gated_model):\n",
        "        base_model_parameters = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
        "        gated_model_parameters = sum(p.numel() for p in gated_model.parameters() if p.requires_grad)\n",
        "        return gated_model_parameters / base_model_parameters\n",
        "\n",
        "    def set_zeros(self, amount=0.5):\n",
        "        with torch.no_grad():\n",
        "            for i, layer in enumerate(self.gate_layers):\n",
        "                gate, _, _ = layer\n",
        "                w = gate.weight\n",
        "                size = int(w.shape[0]*amount)\n",
        "                idxs_samples = random.sample(range(0, w.shape[0]), size)\n",
        "                for i in idxs_samples:\n",
        "                    w[i,:,:] = 0\n",
        "\n",
        "    def prune_model_channels(self, model, amount=0.4, pruning_idxs=None, ):\n",
        "        for i, module in enumerate(NAS.ready_to_prune):\n",
        "            _, module_obj = module\n",
        "            DG = tp.DependencyGraph()\n",
        "            DG.build_dependency(model, example_inputs=torch.randn(1,3,32,32))\n",
        "            \n",
        "            if pruning_idxs is None:\n",
        "                strategy = tp.strategy.L1Strategy() \n",
        "                idxs = strategy(module_obj.weight, amount=0.4)\n",
        "            else: idxs = pruning_idxs[i]\n",
        "\n",
        "            pruning_plan = DG.get_pruning_plan( module_obj, tp.prune_conv, idxs=idxs )\n",
        "            if self.verbose:\n",
        "                print(pruning_plan)\n",
        "            pruning_plan.exec()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IuEyfpy1gHls",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:59.523041Z",
          "iopub.execute_input": "2022-06-09T08:50:59.523527Z",
          "iopub.status.idle": "2022-06-09T08:50:59.555348Z",
          "shell.execute_reply.started": "2022-06-09T08:50:59.523488Z",
          "shell.execute_reply": "2022-06-09T08:50:59.554413Z"
        },
        "trusted": true
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NAS  = NAS_GatedConvolution(verbose=False)\n",
        "# gated_model = ResNet18()\n",
        "# NAS.apply_gates_to_model(gated_model, apply_to_layer_types=[nn.Conv2d])    \n",
        "# NAS.estimate_required_channels(use_mean= 0.8)   \n",
        "# NAS.project_gates_on_model(use_mean= 0.8)   \n",
        "# NAS.remove_gates_from_model()   "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:59.556927Z",
          "iopub.execute_input": "2022-06-09T08:50:59.557316Z",
          "iopub.status.idle": "2022-06-09T08:50:59.567165Z",
          "shell.execute_reply.started": "2022-06-09T08:50:59.557280Z",
          "shell.execute_reply": "2022-06-09T08:50:59.566483Z"
        },
        "trusted": true,
        "id": "qHCbYBeIMv5k"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(ResNet18())"
      ],
      "metadata": {
        "id": "nFXtw0cNnoZ_",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:50:59.568626Z",
          "iopub.execute_input": "2022-06-09T08:50:59.569087Z",
          "iopub.status.idle": "2022-06-09T08:50:59.578405Z",
          "shell.execute_reply.started": "2022-06-09T08:50:59.569051Z",
          "shell.execute_reply": "2022-06-09T08:50:59.577643Z"
        },
        "trusted": true
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "zwl57jMWjnq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args['lr'] = 1e-3\n",
        "args['lambda_penalty'] = 1e-3\n",
        "args['distil_weight'] = 0.8\n",
        "\n",
        "NAS  = NAS_GatedConvolution(verbose=False)\n",
        "\n",
        "# TEACHER\n",
        "teacher_model = ResNet18()\n",
        "resume_checkpoint(teacher_model, 'resnet18_net_e_199', map_location=torch.device(device))\n",
        "\n",
        "# STUDENT\n",
        "student_model = ResNet18()\n",
        "#resume_checkpoint(student_model, 'resnet18_net_e_199', map_location=torch.device(device))\n",
        "# freezeAllLayers(student_model)\n",
        "NAS.apply_gates_to_model(student_model, apply_to_layer_types=[nn.Conv2d])    \n",
        "\n",
        "# DISTILL\n",
        "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=args['lr'],\n",
        "                      momentum=0.7, weight_decay=5e-4)\n",
        "student_optimizer = optim.SGD(student_model.parameters(), lr=args['lr'],\n",
        "                      momentum=0.7, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "distiller = GatedKD(teacher_model, student_model, trainloader, testloader, \n",
        "                    teacher_optimizer, student_optimizer, loss_fn=nn.KLDivLoss(), \n",
        "                    gating_weight=args['lambda_penalty'], gate_layers=NAS.gate_layers, \n",
        "                    distil_weight=args['distil_weight'], log=True, logdir=\"./logs\", device=device)  "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-09T09:52:09.198837Z",
          "iopub.execute_input": "2022-06-09T09:52:09.199514Z",
          "iopub.status.idle": "2022-06-09T09:52:09.811619Z",
          "shell.execute_reply.started": "2022-06-09T09:52:09.199463Z",
          "shell.execute_reply": "2022-06-09T09:52:09.810398Z"
        },
        "trusted": true,
        "id": "AZkLvDz2Mv5l"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training student"
      ],
      "metadata": {
        "id": "J615e7CEMv5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distiller.train_student(epochs=100, save_model_pth='./checkpoint/student_e100.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-09T09:52:10.897791Z",
          "iopub.execute_input": "2022-06-09T09:52:10.898737Z",
          "iopub.status.idle": "2022-06-09T09:56:16.635037Z",
          "shell.execute_reply.started": "2022-06-09T09:52:10.898679Z",
          "shell.execute_reply": "2022-06-09T09:56:16.634096Z"
        },
        "trusted": true,
        "id": "fWW3CWnFMv5l",
        "outputId": "9b23d4fa-029c-4ae3-e481-fbc958d39354",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Student...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2887: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  \"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.3025\n",
            "Epoch: 1, Loss: -25890.24652862549, Accuracy: 0.23678\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.3858\n",
            "Epoch: 2, Loss: -29792.415840148926, Accuracy: 0.33348\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.4358\n",
            "Epoch: 3, Loss: -29822.436561584473, Accuracy: 0.39696\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.4633\n",
            "Epoch: 4, Loss: -29828.41371154785, Accuracy: 0.43654\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.4744\n",
            "Epoch: 5, Loss: -29832.67473602295, Accuracy: 0.4614\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.509\n",
            "Epoch: 6, Loss: -29836.16623687744, Accuracy: 0.47956\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.5181\n",
            "Epoch: 7, Loss: -29839.49284362793, Accuracy: 0.49964\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.5196\n",
            "Epoch: 8, Loss: -29842.55297088623, Accuracy: 0.51728\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.5488\n",
            "Epoch: 9, Loss: -29845.22681427002, Accuracy: 0.5329\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.5589\n",
            "Epoch: 10, Loss: -29847.35433959961, Accuracy: 0.5469\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.5768\n",
            "Epoch: 11, Loss: -29849.825813293457, Accuracy: 0.5601\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.5775\n",
            "Epoch: 12, Loss: -29851.877868652344, Accuracy: 0.57082\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.5993\n",
            "Epoch: 13, Loss: -29854.32469177246, Accuracy: 0.586\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.5957\n",
            "Epoch: 14, Loss: -29856.014755249023, Accuracy: 0.596\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.5919\n",
            "Epoch: 15, Loss: -29858.02712249756, Accuracy: 0.60692\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6233\n",
            "Epoch: 16, Loss: -29859.91106414795, Accuracy: 0.6157\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6364\n",
            "Epoch: 17, Loss: -29862.196380615234, Accuracy: 0.62828\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6431\n",
            "Epoch: 18, Loss: -29863.441009521484, Accuracy: 0.63382\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6465\n",
            "Epoch: 19, Loss: -29865.238410949707, Accuracy: 0.64502\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6577\n",
            "Epoch: 20, Loss: -29866.647163391113, Accuracy: 0.6513\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6679\n",
            "Epoch: 21, Loss: -29868.278495788574, Accuracy: 0.65958\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6714\n",
            "Epoch: 22, Loss: -29868.987533569336, Accuracy: 0.6617\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6486\n",
            "Epoch: 23, Loss: -29870.564781188965, Accuracy: 0.67006\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6595\n",
            "Epoch: 24, Loss: -29871.73949432373, Accuracy: 0.67606\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6801\n",
            "Epoch: 25, Loss: -29873.055793762207, Accuracy: 0.68234\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6615\n",
            "Epoch: 26, Loss: -29874.355430603027, Accuracy: 0.69082\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6941\n",
            "Epoch: 27, Loss: -29875.308044433594, Accuracy: 0.6945\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.7016\n",
            "Epoch: 28, Loss: -29876.102752685547, Accuracy: 0.6978\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.7055\n",
            "Epoch: 29, Loss: -29877.145614624023, Accuracy: 0.7061\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.7049\n",
            "Epoch: 30, Loss: -29877.952590942383, Accuracy: 0.70914\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.714\n",
            "Epoch: 31, Loss: -29878.943786621094, Accuracy: 0.71186\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.6867\n",
            "Epoch: 32, Loss: -29880.07984161377, Accuracy: 0.71746\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.7161\n",
            "Epoch: 33, Loss: -29880.690879821777, Accuracy: 0.72016\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.7223\n",
            "Epoch: 34, Loss: -29881.644943237305, Accuracy: 0.72592\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.7111\n",
            "Epoch: 35, Loss: -29882.294761657715, Accuracy: 0.73\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.7171\n",
            "Epoch: 36, Loss: -29883.209815979004, Accuracy: 0.73518\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.7338\n",
            "Epoch: 37, Loss: -29883.840171813965, Accuracy: 0.73664\n",
            "--------------------------------------------------------------------------------\n",
            "Validation Accuracy: 0.7162\n",
            "Epoch: 38, Loss: -29884.656768798828, Accuracy: 0.73872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NAS.verbose=True\n",
        "NAS.gating_threshold = 1e-2\n",
        "idxs = NAS.estimate_required_channels(use_mean=0.5)   \n",
        "NAS.verbose=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gue92Rs0tRK",
        "outputId": "33784de0-93d8-429b-e80f-5843659ec367"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Layer index  0  | Zeros 0 / 64  | Mean 0.993452787399292  | Std 0.0010497854091227055\n",
            "1 Layer index  1  | Zeros 0 / 64  | Mean 0.9933662414550781  | Std 0.0008041745168156922\n",
            "2 Layer index  2  | Zeros 0 / 64  | Mean 0.9902085661888123  | Std 0.0009862545412033796\n",
            "3 Layer index  3  | Zeros 0 / 64  | Mean 0.9933662414550781  | Std 0.0008041745168156922\n",
            "4 Layer index  4  | Zeros 0 / 64  | Mean 0.9902139902114868  | Std 0.0011929869651794434\n",
            "5 Layer index  5  | Zeros 0 / 128  | Mean 0.9918797016143799  | Std 0.0005994968232698739\n",
            "6 Layer index  6  | Zeros 0 / 128  | Mean 0.9933137893676758  | Std 0.0005438151420094073\n",
            "7 Layer index  7  | Zeros 0 / 128  | Mean 0.9918797016143799  | Std 0.0005994917009957135\n",
            "8 Layer index  8  | Zeros 0 / 128  | Mean 0.9933137893676758  | Std 0.0005437806248664856\n",
            "9 Layer index  9  | Zeros 0 / 128  | Mean 0.9918797016143799  | Std 0.0005994464736431837\n",
            "10 Layer index  10  | Zeros 0 / 256  | Mean 0.9932596683502197  | Std 0.00036435603396967053\n",
            "11 Layer index  11  | Zeros 0 / 256  | Mean 0.993259608745575  | Std 0.0003645544347818941\n",
            "12 Layer index  12  | Zeros 0 / 256  | Mean 0.9926124811172485  | Std 0.00039950187783688307\n",
            "13 Layer index  13  | Zeros 0 / 256  | Mean 0.9932892322540283  | Std 0.00037546345265582204\n",
            "14 Layer index  14  | Zeros 0 / 256  | Mean 0.993302583694458  | Std 0.0004714512324426323\n",
            "15 Layer index  15  | Zeros 0 / 512  | Mean 0.9932764768600464  | Std 0.0002677380107343197\n",
            "16 Layer index  16  | Zeros 0 / 512  | Mean 0.993301510810852  | Std 0.0004933061427436769\n",
            "17 Layer index  17  | Zeros 0 / 512  | Mean 0.9929496645927429  | Std 0.0002783137606456876\n",
            "18 Layer index  18  | Zeros 0 / 512  | Mean 0.9932820796966553  | Std 0.0003378793771844357\n",
            "19 Layer index  19  | Zeros 0 / 512  | Mean 0.9931937456130981  | Std 0.002567945746704936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old Training Step"
      ],
      "metadata": {
        "id": "P0hcPw-0Mv5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NAS  = NAS_GatedConvolution(verbose=False)\n",
        "\n",
        "print('==> Praparing base model..')\n",
        "gated_model = ResNet18()\n",
        "resume_checkpoint(gated_model, 'resnet18_net_e_199', map_location=torch.device(device))\n",
        "freezeAllLayers(gated_model)\n",
        "NAS.apply_gates_to_model(gated_model, apply_to_layer_types=[nn.Conv2d])    \n",
        "#NAS.estimate_required_channels(use_mean= 0.8)    "
      ],
      "metadata": {
        "id": "GIC1A3_gjz4x",
        "outputId": "441be878-6e7f-43ac-895e-d442edcda5eb",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:55:11.208289Z",
          "iopub.status.idle": "2022-06-09T08:55:11.209395Z",
          "shell.execute_reply.started": "2022-06-09T08:55:11.209127Z",
          "shell.execute_reply": "2022-06-09T08:55:11.209158Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Praparing base model..\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(GatedConvolution(n_channels64), 'conv1', ResNet(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels64)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Sequential(\n",
              "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels64)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Sequential(\n",
              "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels64)\n",
              "        )\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Sequential(\n",
              "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels64)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Sequential(\n",
              "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels64)\n",
              "        )\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels128)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Sequential(\n",
              "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels128)\n",
              "        )\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential(\n",
              "          (0): Sequential(\n",
              "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): GatedConvolution(n_channels128)\n",
              "          )\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Sequential(\n",
              "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels128)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Sequential(\n",
              "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels128)\n",
              "        )\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels256)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels256)\n",
              "        )\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential(\n",
              "          (0): Sequential(\n",
              "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): GatedConvolution(n_channels256)\n",
              "          )\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels256)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels256)\n",
              "        )\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels512)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels512)\n",
              "        )\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential(\n",
              "          (0): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): GatedConvolution(n_channels512)\n",
              "          )\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels512)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GatedConvolution(n_channels512)\n",
              "        )\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (linear): Linear(in_features=512, out_features=10, bias=True)\n",
              "  )), (GatedConvolution(n_channels64), 'conv1', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels64)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels64)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential()\n",
              "  )), (GatedConvolution(n_channels64), 'conv2', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels64)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels64)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential()\n",
              "  )), (GatedConvolution(n_channels64), 'conv1', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels64)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels64)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential()\n",
              "  )), (GatedConvolution(n_channels64), 'conv2', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels64)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels64)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential()\n",
              "  )), (GatedConvolution(n_channels128), 'conv1', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels128)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels128)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): GatedConvolution(n_channels128)\n",
              "      )\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )), (GatedConvolution(n_channels128), 'conv2', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels128)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels128)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): GatedConvolution(n_channels128)\n",
              "      )\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )), (GatedConvolution(n_channels128), '0', Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (1): GatedConvolution(n_channels128)\n",
              "    )\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )), (GatedConvolution(n_channels128), 'conv1', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels128)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels128)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential()\n",
              "  )), (GatedConvolution(n_channels128), 'conv2', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels128)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels128)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential()\n",
              "  )), (GatedConvolution(n_channels256), 'conv1', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels256)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels256)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): GatedConvolution(n_channels256)\n",
              "      )\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )), (GatedConvolution(n_channels256), 'conv2', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels256)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels256)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): GatedConvolution(n_channels256)\n",
              "      )\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )), (GatedConvolution(n_channels256), '0', Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (1): GatedConvolution(n_channels256)\n",
              "    )\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )), (GatedConvolution(n_channels256), 'conv1', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels256)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels256)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential()\n",
              "  )), (GatedConvolution(n_channels256), 'conv2', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels256)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels256)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential()\n",
              "  )), (GatedConvolution(n_channels512), 'conv1', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels512)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels512)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): GatedConvolution(n_channels512)\n",
              "      )\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )), (GatedConvolution(n_channels512), 'conv2', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels512)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels512)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): GatedConvolution(n_channels512)\n",
              "      )\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )), (GatedConvolution(n_channels512), '0', Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (1): GatedConvolution(n_channels512)\n",
              "    )\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )), (GatedConvolution(n_channels512), 'conv1', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels512)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels512)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential()\n",
              "  )), (GatedConvolution(n_channels512), 'conv2', BasicBlock(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels512)\n",
              "    )\n",
              "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (1): GatedConvolution(n_channels512)\n",
              "    )\n",
              "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (shortcut): Sequential()\n",
              "  ))]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "print('==> Building model..')\n",
        "net = gated_model.to(device)\n",
        "\n",
        "if device == 'cuda':\n",
        "    # net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "if args['resume']:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    resume_checkpoint(net, args['resume'])"
      ],
      "metadata": {
        "id": "9tKCUxBXjeli",
        "outputId": "f6aa1323-2f6a-4d1c-8bed-c00fa43c9485",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:55:11.215009Z",
          "iopub.status.idle": "2022-06-09T08:55:11.215684Z",
          "shell.execute_reply.started": "2022-06-09T08:55:11.215444Z",
          "shell.execute_reply": "2022-06-09T08:55:11.215466Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lamba_gating_criterion = 1\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)"
      ],
      "metadata": {
        "id": "CE_XA6zijxPE",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:55:11.216870Z",
          "iopub.status.idle": "2022-06-09T08:55:11.217525Z",
          "shell.execute_reply.started": "2022-06-09T08:55:11.217293Z",
          "shell.execute_reply": "2022-06-09T08:55:11.217316Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(epoch, gating_regularization=False, lamba_gating_criterion=0.1):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets) \n",
        "        if gating_regularization:\n",
        "            gating_loss = torch.sum(torch.Tensor([torch.norm(g[0].weight,1) for g in NAS.gate_layers]))\n",
        "            loss += lamba_gating_criterion * gating_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "\n",
        "def test(epoch, save_ckpt='ckpt'):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            progress_bar(batch_idx, len(testloader), 'Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'\n",
        "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, f'./checkpoint/{save_ckpt}.pth')\n",
        "        best_acc = acc\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+50):\n",
        "    train(epoch, gating_regularization=True, lamba_gating_criterion=lamba_gating_criterion)\n",
        "    test(epoch, save_ckpt=f'pretrained_gatednet_e_{epoch}')\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "NPdii3ODkE_U",
        "outputId": "d54b7189-e889-41cd-e619-1fe3f286156d",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:55:11.218884Z",
          "iopub.status.idle": "2022-06-09T08:55:11.219542Z",
          "shell.execute_reply.started": "2022-06-09T08:55:11.219311Z",
          "shell.execute_reply": "2022-06-09T08:55:11.219334Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Batch 390/391 : Loss: 4378.415 | Acc: 99.996% (49998/50000)\n",
            "Batch 99/100 : Test Loss: 0.177 | Test Acc: 95.330% (9533/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n",
            "Batch 390/391 : Loss: 3598.443 | Acc: 99.998% (49999/50000)\n",
            "Batch 99/100 : Test Loss: 0.177 | Test Acc: 95.410% (9541/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 2\n",
            "Batch 390/391 : Loss: 2958.613 | Acc: 99.998% (49999/50000)\n",
            "Batch 99/100 : Test Loss: 0.176 | Test Acc: 95.340% (9534/10000)\n",
            "\n",
            "Epoch: 3\n",
            "Batch 390/391 : Loss: 2436.752 | Acc: 99.994% (49997/50000)\n",
            "Batch 99/100 : Test Loss: 0.172 | Test Acc: 95.210% (9521/10000)\n",
            "\n",
            "Epoch: 4\n",
            "Batch 390/391 : Loss: 2022.416 | Acc: 99.994% (49997/50000)\n",
            "Batch 99/100 : Test Loss: 0.172 | Test Acc: 94.930% (9493/10000)\n",
            "\n",
            "Epoch: 5\n",
            "Batch 390/391 : Loss: 1738.249 | Acc: 99.978% (49989/50000)\n",
            "Batch 99/100 : Test Loss: 0.176 | Test Acc: 94.790% (9479/10000)\n",
            "\n",
            "Epoch: 6\n",
            "Batch 390/391 : Loss: 1595.579 | Acc: 99.974% (49987/50000)\n",
            "Batch 99/100 : Test Loss: 0.183 | Test Acc: 94.690% (9469/10000)\n",
            "\n",
            "Epoch: 7\n",
            "Batch 390/391 : Loss: 1518.156 | Acc: 99.958% (49979/50000)\n",
            "Batch 99/100 : Test Loss: 0.184 | Test Acc: 94.620% (9462/10000)\n",
            "\n",
            "Epoch: 8\n",
            "Batch 390/391 : Loss: 1464.749 | Acc: 99.964% (49982/50000)\n",
            "Batch 99/100 : Test Loss: 0.187 | Test Acc: 94.470% (9447/10000)\n",
            "\n",
            "Epoch: 9\n",
            "Batch 390/391 : Loss: 1421.608 | Acc: 99.966% (49983/50000)\n",
            "Batch 99/100 : Test Loss: 0.192 | Test Acc: 94.540% (9454/10000)\n",
            "\n",
            "Epoch: 10\n",
            "Batch 390/391 : Loss: 1388.192 | Acc: 99.902% (49951/50000)\n",
            "Batch 99/100 : Test Loss: 0.191 | Test Acc: 94.460% (9446/10000)\n",
            "\n",
            "Epoch: 11\n",
            "Batch 390/391 : Loss: 1356.321 | Acc: 99.906% (49953/50000)\n",
            "Batch 99/100 : Test Loss: 0.194 | Test Acc: 94.340% (9434/10000)\n",
            "\n",
            "Epoch: 12\n",
            "Batch 390/391 : Loss: 1327.602 | Acc: 99.926% (49963/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.480% (9448/10000)\n",
            "\n",
            "Epoch: 13\n",
            "Batch 390/391 : Loss: 1302.908 | Acc: 99.920% (49960/50000)\n",
            "Batch 99/100 : Test Loss: 0.197 | Test Acc: 94.180% (9418/10000)\n",
            "\n",
            "Epoch: 14\n",
            "Batch 390/391 : Loss: 1283.838 | Acc: 99.898% (49949/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.520% (9452/10000)\n",
            "\n",
            "Epoch: 15\n",
            "Batch 390/391 : Loss: 1265.615 | Acc: 99.916% (49958/50000)\n",
            "Batch 99/100 : Test Loss: 0.191 | Test Acc: 94.470% (9447/10000)\n",
            "\n",
            "Epoch: 16\n",
            "Batch 390/391 : Loss: 1248.533 | Acc: 99.932% (49966/50000)\n",
            "Batch 99/100 : Test Loss: 0.192 | Test Acc: 94.630% (9463/10000)\n",
            "\n",
            "Epoch: 17\n",
            "Batch 390/391 : Loss: 1235.532 | Acc: 99.920% (49960/50000)\n",
            "Batch 99/100 : Test Loss: 0.191 | Test Acc: 94.470% (9447/10000)\n",
            "\n",
            "Epoch: 18\n",
            "Batch 390/391 : Loss: 1222.617 | Acc: 99.926% (49963/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.430% (9443/10000)\n",
            "\n",
            "Epoch: 19\n",
            "Batch 390/391 : Loss: 1213.489 | Acc: 99.886% (49943/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.450% (9445/10000)\n",
            "\n",
            "Epoch: 20\n",
            "Batch 390/391 : Loss: 1203.024 | Acc: 99.920% (49960/50000)\n",
            "Batch 99/100 : Test Loss: 0.194 | Test Acc: 94.410% (9441/10000)\n",
            "\n",
            "Epoch: 21\n",
            "Batch 390/391 : Loss: 1195.375 | Acc: 99.932% (49966/50000)\n",
            "Batch 99/100 : Test Loss: 0.189 | Test Acc: 94.550% (9455/10000)\n",
            "\n",
            "Epoch: 22\n",
            "Batch 390/391 : Loss: 1187.779 | Acc: 99.934% (49967/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.580% (9458/10000)\n",
            "\n",
            "Epoch: 23\n",
            "Batch 390/391 : Loss: 1181.899 | Acc: 99.914% (49957/50000)\n",
            "Batch 99/100 : Test Loss: 0.188 | Test Acc: 94.540% (9454/10000)\n",
            "\n",
            "Epoch: 24\n",
            "Batch 390/391 : Loss: 1176.681 | Acc: 99.926% (49963/50000)\n",
            "Batch 99/100 : Test Loss: 0.186 | Test Acc: 94.450% (9445/10000)\n",
            "\n",
            "Epoch: 25\n",
            "Batch 390/391 : Loss: 1171.457 | Acc: 99.940% (49970/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.380% (9438/10000)\n",
            "\n",
            "Epoch: 26\n",
            "Batch 390/391 : Loss: 1166.736 | Acc: 99.916% (49958/50000)\n",
            "Batch 99/100 : Test Loss: 0.191 | Test Acc: 94.430% (9443/10000)\n",
            "\n",
            "Epoch: 27\n",
            "Batch 390/391 : Loss: 1163.357 | Acc: 99.924% (49962/50000)\n",
            "Batch 99/100 : Test Loss: 0.189 | Test Acc: 94.520% (9452/10000)\n",
            "\n",
            "Epoch: 28\n",
            "Batch 390/391 : Loss: 1160.623 | Acc: 99.912% (49956/50000)\n",
            "Batch 99/100 : Test Loss: 0.191 | Test Acc: 94.430% (9443/10000)\n",
            "\n",
            "Epoch: 29\n",
            "Batch 390/391 : Loss: 1157.334 | Acc: 99.926% (49963/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.360% (9436/10000)\n",
            "\n",
            "Epoch: 30\n",
            "Batch 390/391 : Loss: 1154.205 | Acc: 99.920% (49960/50000)\n",
            "Batch 99/100 : Test Loss: 0.193 | Test Acc: 94.340% (9434/10000)\n",
            "\n",
            "Epoch: 31\n",
            "Batch 390/391 : Loss: 1151.586 | Acc: 99.900% (49950/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.520% (9452/10000)\n",
            "\n",
            "Epoch: 32\n",
            "Batch 390/391 : Loss: 1148.809 | Acc: 99.926% (49963/50000)\n",
            "Batch 99/100 : Test Loss: 0.193 | Test Acc: 94.420% (9442/10000)\n",
            "\n",
            "Epoch: 33\n",
            "Batch 390/391 : Loss: 1147.382 | Acc: 99.920% (49960/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.490% (9449/10000)\n",
            "\n",
            "Epoch: 34\n",
            "Batch 390/391 : Loss: 1145.816 | Acc: 99.922% (49961/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.520% (9452/10000)\n",
            "\n",
            "Epoch: 35\n",
            "Batch 390/391 : Loss: 1145.035 | Acc: 99.882% (49941/50000)\n",
            "Batch 99/100 : Test Loss: 0.189 | Test Acc: 94.510% (9451/10000)\n",
            "\n",
            "Epoch: 36\n",
            "Batch 390/391 : Loss: 1143.459 | Acc: 99.926% (49963/50000)\n",
            "Batch 99/100 : Test Loss: 0.189 | Test Acc: 94.490% (9449/10000)\n",
            "\n",
            "Epoch: 37\n",
            "Batch 390/391 : Loss: 1142.317 | Acc: 99.916% (49958/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.510% (9451/10000)\n",
            "\n",
            "Epoch: 38\n",
            "Batch 390/391 : Loss: 1141.302 | Acc: 99.940% (49970/50000)\n",
            "Batch 99/100 : Test Loss: 0.189 | Test Acc: 94.580% (9458/10000)\n",
            "\n",
            "Epoch: 39\n",
            "Batch 390/391 : Loss: 1140.787 | Acc: 99.940% (49970/50000)\n",
            "Batch 99/100 : Test Loss: 0.188 | Test Acc: 94.560% (9456/10000)\n",
            "\n",
            "Epoch: 40\n",
            "Batch 390/391 : Loss: 1140.110 | Acc: 99.934% (49967/50000)\n",
            "Batch 99/100 : Test Loss: 0.188 | Test Acc: 94.570% (9457/10000)\n",
            "\n",
            "Epoch: 41\n",
            "Batch 390/391 : Loss: 1139.234 | Acc: 99.922% (49961/50000)\n",
            "Batch 99/100 : Test Loss: 0.189 | Test Acc: 94.580% (9458/10000)\n",
            "\n",
            "Epoch: 42\n",
            "Batch 390/391 : Loss: 1138.686 | Acc: 99.914% (49957/50000)\n",
            "Batch 99/100 : Test Loss: 0.189 | Test Acc: 94.490% (9449/10000)\n",
            "\n",
            "Epoch: 43\n",
            "Batch 390/391 : Loss: 1138.387 | Acc: 99.930% (49965/50000)\n",
            "Batch 99/100 : Test Loss: 0.188 | Test Acc: 94.620% (9462/10000)\n",
            "\n",
            "Epoch: 44\n",
            "Batch 390/391 : Loss: 1138.105 | Acc: 99.940% (49970/50000)\n",
            "Batch 99/100 : Test Loss: 0.192 | Test Acc: 94.460% (9446/10000)\n",
            "\n",
            "Epoch: 45\n",
            "Batch 390/391 : Loss: 1137.829 | Acc: 99.942% (49971/50000)\n",
            "Batch 99/100 : Test Loss: 0.188 | Test Acc: 94.600% (9460/10000)\n",
            "\n",
            "Epoch: 46\n",
            "Batch 390/391 : Loss: 1137.725 | Acc: 99.934% (49967/50000)\n",
            "Batch 99/100 : Test Loss: 0.189 | Test Acc: 94.530% (9453/10000)\n",
            "\n",
            "Epoch: 47\n",
            "Batch 390/391 : Loss: 1137.629 | Acc: 99.918% (49959/50000)\n",
            "Batch 99/100 : Test Loss: 0.189 | Test Acc: 94.590% (9459/10000)\n",
            "\n",
            "Epoch: 48\n",
            "Batch 390/391 : Loss: 1137.586 | Acc: 99.918% (49959/50000)\n",
            "Batch 99/100 : Test Loss: 0.190 | Test Acc: 94.530% (9453/10000)\n",
            "\n",
            "Epoch: 49\n",
            "Batch 390/391 : Loss: 1137.565 | Acc: 99.920% (49960/50000)\n",
            "Batch 99/100 : Test Loss: 0.189 | Test Acc: 94.570% (9457/10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gate_weight = ([g[0].weight for g in NAS.gate_layers])\n",
        "old_l = NAS.gate_layers\n",
        "NAS.verbose=True\n",
        "NAS.gating_threshold = 1e-2\n",
        "idxs = NAS.estimate_required_channels(use_mean=0.3)   \n",
        "NAS.verbose=False\n",
        "#NAS.calc_improvement(ResNet18(), gated_model)\n",
        "# new_model = ResNet18()\n",
        "# NAS.prune_model_channels(new_model, pruning_idxs=idxs)"
      ],
      "metadata": {
        "id": "Wx5yrMc0N9e9",
        "outputId": "a93d9e6c-e388-45d2-8e84-83e6559bcb13",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:55:11.220731Z",
          "iopub.status.idle": "2022-06-09T08:55:11.221394Z",
          "shell.execute_reply.started": "2022-06-09T08:55:11.221162Z",
          "shell.execute_reply": "2022-06-09T08:55:11.221184Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Layer index  0  | Zeros 0 / 64  | Mean 0.010490129701793194  | Std 0.005531997419893742\n",
            "1 Layer index  1  | Zeros 5 / 64  | Mean 0.056963805109262466  | Std 0.026103349402546883\n",
            "2 Layer index  2  | Zeros 9 / 64  | Mean 0.07979716360569  | Std 0.03785810247063637\n",
            "3 Layer index  3  | Zeros 25 / 64  | Mean 0.03719443082809448  | Std 0.033819276839494705\n",
            "4 Layer index  4  | Zeros 24 / 64  | Mean 0.0835062637925148  | Std 0.0638878270983696\n",
            "5 Layer index  5  | Zeros 5 / 128  | Mean 0.04898415505886078  | Std 0.017910894006490707\n",
            "6 Layer index  6  | Zeros 3 / 128  | Mean 0.09449909627437592  | Std 0.03006417490541935\n",
            "7 Layer index  7  | Zeros 55 / 128  | Mean 0.054971545934677124  | Std 0.04958360642194748\n",
            "8 Layer index  8  | Zeros 23 / 128  | Mean 0.05743551254272461  | Std 0.03022054024040699\n",
            "9 Layer index  9  | Zeros 34 / 128  | Mean 0.10885466635227203  | Std 0.07703981548547745\n",
            "10 Layer index  10  | Zeros 0 / 256  | Mean 0.07427909970283508  | Std 0.01823461428284645\n",
            "11 Layer index  11  | Zeros 0 / 256  | Mean 0.15927249193191528  | Std 0.025093108415603638\n",
            "12 Layer index  12  | Zeros 158 / 256  | Mean 0.048243194818496704  | Std 0.0642697811126709\n",
            "13 Layer index  13  | Zeros 14 / 256  | Mean 0.11880378425121307  | Std 0.03839336335659027\n",
            "14 Layer index  14  | Zeros 118 / 256  | Mean 0.13352078199386597  | Std 0.1245293989777565\n",
            "15 Layer index  15  | Zeros 93 / 512  | Mean 0.13604165613651276  | Std 0.0685846284031868\n",
            "16 Layer index  16  | Zeros 185 / 512  | Mean 0.1907477080821991  | Std 0.14664064347743988\n",
            "17 Layer index  17  | Zeros 0 / 512  | Mean 0.018484733998775482  | Std 0.04990386962890625\n",
            "18 Layer index  18  | Zeros 152 / 512  | Mean 0.16395825147628784  | Std 0.1103195771574974\n",
            "19 Layer index  19  | Zeros 81 / 512  | Mean 0.36626312136650085  | Std 0.17106442153453827\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-14ecea300bff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#NAS.calc_improvement(ResNet18(), gated_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mNAS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_model_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruning_idxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-822f56b5aada>\u001b[0m in \u001b[0;36mprune_model_channels\u001b[0;34m(self, model, amount, pruning_idxs)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpruning_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mpruning_plan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pruning_plan\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodule_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midxs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpruning_plan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_pruning/dependency.py\u001b[0m in \u001b[0;36mget_pruning_plan\u001b[0;34m(self, module, pruning_fn, idxs)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mplan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPruningPlan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m#  the user pruning operation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0mroot_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_to_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0mplan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_plan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpruning_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruning_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = gated_model.to(device)\n",
        "test(epoch, save_ckpt=f'pretrained_gatednet_e_prova')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2RowA1q2ID3",
        "outputId": "121858f4-6368-45a9-8d8a-e610e5584f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 99/100 : Test Loss: 239542481526456.312 | Test Acc: 10.470% (1047/10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NAS.calc_improvement(ResNet18(), gated_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHfN2Wpq2VXn",
        "outputId": "f3553fa8-493a-4467-c2fe-621008edadf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN BASE\n",
        "# Epoch: 4\n",
        "# Batch 390/391 : Loss: 0.895 | Acc: 68.284% (34142/50000)\n",
        "# Batch 99/100 : Test Loss: 0.918 | Test Acc: 67.270% (6727/10000)\n",
        "\n",
        "# Epoch: 5\n",
        "# Batch 390/391 : Loss: 0.768 | Acc: 73.060% (36530/50000)\n",
        "# Batch 99/100 : Test Loss: 0.735 | Test Acc: 74.720% (7472/10000)\n",
        "\n",
        "# ...\n",
        "\n",
        "# Epoch: 10\n",
        "# Batch 390/391 : Loss: 0.507 | Acc: 82.666% (41333/50000)\n",
        "# Batch 99/100 : Test Loss: 0.672 | Test Acc: 77.910% (7791/10000)\n",
        "\n",
        "# Epoch: 11\n",
        "# Batch 390/391 : Loss: 0.487 | Acc: 83.324% (41662/50000)\n",
        "# Batch 99/100 : Test Loss: 0.723 | Test Acc: 75.570% (7557/10000)\n",
        "\n",
        "# ....\n",
        "\n",
        "# Epoch: 23\n",
        "# Batch 390/391 : Loss: 0.375 | Acc: 87.314% (43657/50000)\n",
        "# Batch 99/100 : Test Loss: 0.508 | Test Acc: 83.310% (8331/10000)\n",
        "\n",
        "# Epoch: 24\n",
        "# Batch 390/391 : Loss: 0.370 | Acc: 87.426% (43713/50000)\n",
        "# Batch 99/100 : Test Loss: 0.502 | Test Acc: 83.690% (8369/10000)\n",
        "# Saving..\n",
        "\n",
        "# Epoch: 25\n",
        "# Batch 390/391 : Loss: 0.370 | Acc: 87.196% (43598/50000)\n",
        "# Batch 99/100 : Test Loss: 0.468 | Test Acc: 84.700% (8470/10000)\n",
        "# Saving.."
      ],
      "metadata": {
        "id": "w_oLe-vcI_Kh",
        "execution": {
          "iopub.status.busy": "2022-06-09T08:55:11.222598Z",
          "iopub.status.idle": "2022-06-09T08:55:11.223257Z",
          "shell.execute_reply.started": "2022-06-09T08:55:11.223031Z",
          "shell.execute_reply": "2022-06-09T08:55:11.223054Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a0uK49lRI9z8"
      }
    }
  ]
}